{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments='Comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(data):\n",
    "    sentences=sent_tokenize(data)\n",
    "    return sentences\n",
    "\n",
    "def tokenize_word(data):\n",
    "    words=word_tokenize(data)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[comments]=data[comments].apply(tokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_data=data[comments]\n",
    "sentences=[]\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "for data in unigram_data:\n",
    "    for sent in data:\n",
    "        new_sent=sent.lower()\n",
    "        sentences.append(new_sent.translate(translator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sent=sentences\n",
    "bigram_sent=[]\n",
    "trigram_sent=[]\n",
    "quadgram_sent=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    bigram_sent.append(sent)\n",
    "    trigram_sent.append(sent)\n",
    "    quadgram_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_words=[]\n",
    "for sent in unigram_sent:\n",
    "    words=word_tokenize(sent)\n",
    "    unigram_words.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_words=unigram_words.copy()\n",
    "trigram_words=unigram_words.copy()\n",
    "quadgram_words = unigram_words.copy()\n",
    "\n",
    "for sent in bigram_words:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0,'<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in trigram_words:\n",
    "    sent.append('</s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0,'<s>')\n",
    "    sent.insert(0,'<s>')\n",
    "\n",
    "for sent in quadgram_words:\n",
    "    sent.append('</s>')\n",
    "    sent.append('</s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0,'<s>')\n",
    "    sent.insert(0,'<s>')\n",
    "    sent.insert(0,'<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_sent='<s>'\n",
    "end_sent='</s>'\n",
    "\n",
    "class unigram_model():\n",
    "  def __init__(self,sentences):\n",
    "    self.unigram_frequencies=dict()\n",
    "    self.vocabulary=set()\n",
    "    self.corpus_size=0\n",
    "    for sentence in sentences:\n",
    "      for word in sentence:\n",
    "        self.unigram_frequencies[word]=self.unigram_frequencies.get(word,0)+1\n",
    "        if word!=start_sent or  word!=end_sent:  \n",
    "         self.corpus_size+=1\n",
    "        if word not in self.vocabulary:\n",
    "         self.vocabulary.add(word)\n",
    "    self.vocab_size=len(self.unigram_frequencies)-2 #Not including start and end of sentence in vocabulary\n",
    "\n",
    "  def calculate_probability(self,word):\n",
    "    if word not in self.vocabulary:\n",
    "      return 0\n",
    "    return float(self.unigram_frequencies[word])/float(self.corpus_size)\n",
    "\n",
    "  def calculate_perplexity(self,word):\n",
    "    prob_word=self.calculate_probability(word)\n",
    "    if(prob_word==0):\n",
    "      return 10**(7)\n",
    "    perplexity=np.log2(float(1/prob_word))*(float(1/self.corpus_size))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "class bigram_model(unigram_model):\n",
    "  def __init__(self,sentences):\n",
    "    unigram_model.__init__(self,sentences)\n",
    "    self.bigram_frequencies=dict()\n",
    "    self.total_bigrams=0\n",
    "    for sentence in sentences:\n",
    "      prev_word=sentence[0]\n",
    "      for word in sentence[1:]:\n",
    "        self.bigram_frequencies[(prev_word,word)]=self.bigram_frequencies.get((prev_word,word),0)+1\n",
    "        self.total_bigrams+=1\n",
    "        prev_word=word\n",
    "\n",
    "    self.total_bigram_words=len(self.bigram_frequencies)\n",
    "\n",
    "  def calculate_probability(self,prev_word,word):\n",
    "    a=self.bigram_frequencies.get((prev_word,word),0)\n",
    "    b=self.unigram_frequencies.get(prev_word,0)\n",
    "\n",
    "    if b==0:\n",
    "      return 0\n",
    "    return (float(a)/float(b))*unigram_model.calculate_probability(self,prev_word)\n",
    "\n",
    "  def calculate_perplexity(self,prev_word,word):\n",
    "    prob_word=self.calculate_probability(prev_word,word)\n",
    "    if(prob_word==0):\n",
    "      return 10**(7)\n",
    "    perplexity=np.log2(float((1/prob_word)))*(float(1/self.total_bigrams))\n",
    "    return perplexity\n",
    "\n",
    "class trigram_model(bigram_model):\n",
    "    def __init__(self, sentences):\n",
    "        bigram_model.__init__(self, sentences)\n",
    "        self.trigram_frequencies = {}\n",
    "        self.total_trigrams=0\n",
    "        for sentence in sentences:\n",
    "            prev_word1 = sentence[0]\n",
    "            prev_word2 = sentence[1]\n",
    "            for word in sentence[2:]:\n",
    "              self.trigram_frequencies[(prev_word1, prev_word2, word)] = self.trigram_frequencies.get((prev_word1, prev_word2, word), 0) + 1\n",
    "              prev_word1 = prev_word2\n",
    "              prev_word2 = word\n",
    "              self.total_trigrams+=1\n",
    "\n",
    "        self.total_trigram_words = len(self.trigram_frequencies)\n",
    "\n",
    "    def calculate_probability(self, prev_word1, prev_word2, word):\n",
    "        trigram_frequency = self.trigram_frequencies.get((prev_word1, prev_word2, word), 0)\n",
    "        bigram_frequency = self.bigram_frequencies.get((prev_word2, word), 0)\n",
    "\n",
    "        if bigram_frequency == 0:\n",
    "            return 0\n",
    "        return (float(trigram_frequency) / float(bigram_frequency))*bigram_model.calculate_probability(self,prev_word1,prev_word2)\n",
    "\n",
    "    def calculate_perplexity(self,prev_word1, prev_word2, word):\n",
    "      prob_word=self.calculate_probability(prev_word1, prev_word2, word)\n",
    "      if(prob_word==0):\n",
    "        return 10**(7) #float('inf')\n",
    "      perplexity=np.log2(float((1/prob_word)))*(float(1/self.total_trigrams))\n",
    "      return perplexity\n",
    "\n",
    "\n",
    "class quadgram_model(trigram_model):\n",
    "    def __init__(self, sentences):\n",
    "        trigram_model.__init__(self, sentences)\n",
    "        self.quadgram_frequencies = {}\n",
    "        self.total_quadgrams=0\n",
    "        for sentence in sentences:\n",
    "            prev_word1 = sentence[0]\n",
    "            prev_word2 = sentence[1]\n",
    "            prev_word3 = sentence[2]\n",
    "            for word in sentence[3:]:\n",
    "                quadgram = (prev_word1,prev_word2,prev_word3,word)\n",
    "                self.quadgram_frequencies[quadgram] = self.quadgram_frequencies.get(quadgram, 0) + 1\n",
    "                prev_word1 = prev_word2\n",
    "                prev_word2 = prev_word3\n",
    "                prev_word3 = word\n",
    "                self.total_quadgrams+=1\n",
    "\n",
    "        self.total_quadgram_words = len(self.quadgram_frequencies)\n",
    "\n",
    "\n",
    "    def calculate_probability(self, prev_word1, prev_word2, prev_word3, word):\n",
    "        quadgram_frequency = self.quadgram_frequencies.get((prev_word1, prev_word2, prev_word3, word), 0)\n",
    "        trigram_frequency = self.trigram_frequencies.get((prev_word1, prev_word2, prev_word3), 0)\n",
    "\n",
    "        if trigram_frequency == 0:\n",
    "            return 0\n",
    "        return (float(quadgram_frequency) / float(trigram_frequency))*trigram_model.calculate_probability(self,prev_word1,prev_word2,prev_word3)\n",
    "    \n",
    "    def calculate_perplexity(self,prev_word1, prev_word2, prev_word3, word):\n",
    "      prob_word=self.calculate_probability(prev_word1, prev_word2, prev_word3, word)\n",
    "      if(prob_word==0):\n",
    "        return 10**(7)\n",
    "      perplexity=np.log2(float((1/prob_word)))*(float(1/self.total_quadgrams))\n",
    "      return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_dataset.csv')\n",
    "test_comments='Comment'\n",
    "\n",
    "test_data[test_comments]=test_data[test_comments].apply(tokenize_sent)\n",
    "test_unigram_data=test_data[test_comments]\n",
    "test_sentences=[]\n",
    "for data in test_unigram_data:\n",
    "    for sent in data:\n",
    "        new_sent=sent.lower()\n",
    "        test_sentences.append(new_sent.translate(translator))\n",
    "\n",
    "test_unigram_sent=test_sentences\n",
    "test_unigram_words=[]\n",
    "for sent in test_unigram_sent:\n",
    "    words=word_tokenize(sent)\n",
    "    test_unigram_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIGRAM_MODEL=unigram_model(unigram_words)\n",
    "\n",
    "unigram_perplex=0\n",
    "count=0\n",
    "for data in test_unigram_words:\n",
    "    for word in data:\n",
    "        count+=1\n",
    "        unigram_perplex+=UNIGRAM_MODEL.calculate_perplexity(word)\n",
    "avg_perplex=unigram_perplex/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145340.5528644213"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1758750.3513784565"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIGRAM_MODEL=bigram_model(bigram_words)\n",
    "bigram_perplex=0\n",
    "count=0\n",
    "for data in test_unigram_words:\n",
    "    for i in range(len(data)-1):\n",
    "        count+=1\n",
    "        bigram_perplex+=BIGRAM_MODEL.calculate_perplexity(data[i],data[i+1])\n",
    "Avg_perplex=bigram_perplex/count\n",
    "Avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Gram(unigram_model):\n",
    "    def __init__(self,sentences,n):\n",
    "        if(n==1):\n",
    "            self.model=unigram_model(sentences)\n",
    "        if(n==2):\n",
    "            self.model=bigram_model(sentences)\n",
    "        if(n==3):\n",
    "            self.model=trigram_model(sentences)\n",
    "        if(n==4):\n",
    "            self.model=quadgram_model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uni_gram_model=N_Gram(unigram_words,1)\n",
    "test_bi_gram_model=N_Gram(bigram_words,2)\n",
    "test_tri_gram_model=N_Gram(trigram_words,3)\n",
    "test_quad_gram_model=N_Gram(quadgram_words,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021763939748329936\n",
      "0.0005819327752296191\n",
      "1.8842512452678341e-06\n",
      "8.696544208928466e-07\n"
     ]
    }
   ],
   "source": [
    "print(test_uni_gram_model.model.calculate_probability('the'))\n",
    "print(test_bi_gram_model.model.calculate_probability('is','the'))\n",
    "print(test_tri_gram_model.model.calculate_probability('seems','to','me'))\n",
    "print(test_quad_gram_model.model.calculate_probability('seems','to','me','that'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
