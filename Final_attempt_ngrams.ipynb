{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as  plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments='Comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(data):\n",
    "    sentences=sent_tokenize(data)\n",
    "    return sentences\n",
    "\n",
    "def tokenize_word(data):\n",
    "    words=word_tokenize(data)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[comments]=data[comments].apply(tokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_data=data[comments]\n",
    "sentences=[]\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "for data in unigram_data:\n",
    "    for sent in data:\n",
    "        new_sent=sent.lower()\n",
    "        sentences.append(new_sent.translate(translator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sent=sentences\n",
    "bigram_sent=[]\n",
    "trigram_sent=[]\n",
    "quadgram_sent=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    bigram_sent.append(sent)\n",
    "    trigram_sent.append(sent)\n",
    "    quadgram_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_words=[]\n",
    "for sent in unigram_sent:\n",
    "    words=word_tokenize(sent)\n",
    "    unigram_words.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_words=unigram_words.copy()\n",
    "trigram_words=unigram_words.copy()\n",
    "quadgram_words = unigram_words.copy()\n",
    "\n",
    "for sent in bigram_words:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0,'<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in trigram_words:\n",
    "    sent.append('</s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0,'<s>')\n",
    "    sent.insert(0,'<s>')\n",
    "\n",
    "for sent in quadgram_words:\n",
    "    sent.append('</s>')\n",
    "    sent.append('</s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0,'<s>')\n",
    "    sent.insert(0,'<s>')\n",
    "    sent.insert(0,'<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_sent = \"<s>\"\n",
    "end_sent = \"</s>\"\n",
    "\n",
    "\n",
    "class unigram_model:\n",
    "    def __init__(self, sentences):\n",
    "        self.unigram_frequencies = dict()\n",
    "        self.vocabulary = set()\n",
    "        self.corpus_size = 0\n",
    "        self.good_turing_count=dict()\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigram_frequencies[word] = (\n",
    "                    self.unigram_frequencies.get(word, 0) + 1\n",
    "                )\n",
    "                if word != start_sent or word != end_sent:\n",
    "                    self.corpus_size += 1\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary.add(word)\n",
    "        self.vocab_size = (\n",
    "            len(self.unigram_frequencies) - 2\n",
    "        )  # Not including start and end of sentence in vocabulary\n",
    "\n",
    "    def calculate_probability(self, word):\n",
    "        if word not in self.vocabulary:\n",
    "            return 0\n",
    "        return float(self.unigram_frequencies[word]) / float(self.corpus_size)\n",
    "\n",
    "    def calculate_perplexity(self, word):\n",
    "        prob_word = self.calculate_probability(word)\n",
    "        if prob_word == 0:\n",
    "            return 10 ** (7)\n",
    "        perplexity = np.log2(float(1 / prob_word)) * (float(1 / self.corpus_size))\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_probability_smooth(self, word, k):\n",
    "        return float(self.unigram_frequencies[word] + k) / (\n",
    "            float(self.corpus_size) + k * float(self.vocab_size)\n",
    "        )\n",
    "\n",
    "    def calculate_probability_good_turing(self):\n",
    "        # freq_to_words_dict : {Key:Number of words occured that many times, Value:List of words that occured that many times}\n",
    "        freq_to_words_dict = dict()\n",
    "\n",
    "        for word in self.unigram_frequencies:\n",
    "            freq_to_words_dict[self.unigram_frequencies[word]] = freq_to_words_dict.get(\n",
    "                self.unigram_frequencies[word], []\n",
    "            ) + [word]\n",
    "        freq_to_words_dict[0] = [\"<unk>\"]\n",
    "        # new_word_count : {Key:Word, Value:New count of word according to good turing}\n",
    "        new_word_count = dict()\n",
    "        keys = sorted(freq_to_words_dict.keys())\n",
    "        for i in range(1, len(keys) - 1):\n",
    "            for word in freq_to_words_dict[keys[i]]:\n",
    "                new_word_count[word] = (\n",
    "                    (self.unigram_frequencies[word] + 1) * keys[i + 1] / keys[i]\n",
    "                )\n",
    "        # Keeping the word count same for the most frequent words\n",
    "        for word in freq_to_words_dict[keys[-1]]:\n",
    "            new_word_count[word] = self.unigram_frequencies[word]\n",
    "        # Calculating the probability of <unk>, <unk> is which has not appeared in the corpus\n",
    "        new_word_count[\"<unk>\"] = len(freq_to_words_dict[1]) / self.corpus_size\n",
    "        self.good_turing_count = new_word_count\n",
    "    \n",
    "    def calculate_probability_good_smooth(self, word):\n",
    "        new_word_count = self.good_turing_count\n",
    "        if word not in new_word_count:\n",
    "            return new_word_count[\"<unk>\"]\n",
    "        return new_word_count[word] / self.corpus_size\n",
    "\n",
    "\n",
    "class bigram_model(unigram_model):\n",
    "    def __init__(self, sentences):\n",
    "        unigram_model.__init__(self, sentences)\n",
    "        self.bigram_frequencies = dict()\n",
    "        self.second_word_app=dict()\n",
    "        self.first_word_app=dict()\n",
    "        self.total_bigrams = 0\n",
    "        for sentence in sentences:\n",
    "            prev_word = sentence[0]\n",
    "            for word in sentence[1:]:\n",
    "                self.bigram_frequencies[(prev_word, word)] = (\n",
    "                    self.bigram_frequencies.get((prev_word, word), 0) + 1\n",
    "                )\n",
    "                self.total_bigrams += 1\n",
    "                prev_word = word\n",
    "\n",
    "        self.total_bigram_words = len(self.bigram_frequencies)\n",
    "\n",
    "    def calculate_probability(self, prev_word, word):\n",
    "        a = self.bigram_frequencies.get((prev_word, word), 0)\n",
    "        b = self.unigram_frequencies.get(prev_word, 0)\n",
    "\n",
    "        if b == 0:\n",
    "            return 0\n",
    "        return float(a) / float(b)\n",
    "\n",
    "    def calculate_perplexity(self, prev_word, word):\n",
    "        prob_word = self.calculate_probability(prev_word, word)\n",
    "        if prob_word == 0:\n",
    "            return 10 ** (7)\n",
    "        perplexity = np.log2(float((1 / prob_word))) * (float(1 / self.total_bigrams))\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_probability_smooth(self, prev_word, word, k):\n",
    "        a = self.bigram_frequencies.get((prev_word, word), 0)\n",
    "        b = self.unigram_frequencies.get(prev_word, 0)\n",
    "\n",
    "        return (float(a) + k) / (float(b) + k * self.vocab_size)\n",
    "\n",
    "    def calculate_self_second_word(self,):\n",
    "        count_word_appearance = 0\n",
    "        for key in self.bigram_frequencies:\n",
    "            self.second_word_app[key[1]] = self.second_word_app.get(key[1], 0) + 1\n",
    "        # return count_word_appearance / (len(self.bigram_frequencies))\n",
    "    def calculate_self_first_word(self,):\n",
    "        count_word_appearance = 0\n",
    "        for key in self.bigram_frequencies:\n",
    "            self.first_word_app[key[0]] = self.first_word_app.get(key[1], 0) + 1\n",
    "\n",
    "    def initialise_kneser_ney(self):\n",
    "        self.calculate_self_second_word()\n",
    "        self.calculate_self_first_word()\n",
    "    def calculate_continuation_probability(self, word):\n",
    "        return float(self.second_word_app[word]) / float(len(self.bigram_frequencies))\n",
    "\n",
    "    def kneser_ney_smoothing(self, prev_word, word, d=0.75):\n",
    "        # d is the discounting factor, usually 0.75 in slides\n",
    "        if(word not in self.unigram_frequencies.keys() or prev_word not in self.unigram_frequencies.keys()):\n",
    "            return d/(self.corpus_size)\n",
    "        lambda_val = d / (self.unigram_frequencies[prev_word])\n",
    "        count = 0\n",
    "        lambda_val *= self.first_word_app[prev_word]\n",
    "        term_2 = lambda_val * self.calculate_continuation_probability(word)\n",
    "        if((prev_word, word) in self.bigram_frequencies.keys()):\n",
    "            term1 = max(self.bigram_frequencies[(prev_word, word)] - d, 0)\n",
    "            term1 /= self.unigram_frequencies[prev_word]\n",
    "        else:\n",
    "            if(word in self.unigram_frequencies.keys()):\n",
    "                term1 = max(self.unigram_frequencies[word] - d, 0)\n",
    "                term1 /= self.corpus_size\n",
    "            else:\n",
    "                term1 = 0\n",
    "        return term1 + term_2\n",
    "\n",
    "\n",
    "class trigram_model(bigram_model):\n",
    "    def __init__(self, sentences):\n",
    "        bigram_model.__init__(self, sentences)\n",
    "        self.trigram_frequencies = {}\n",
    "        self.total_trigrams = 0\n",
    "        for sentence in sentences:\n",
    "            prev_word1 = sentence[0]\n",
    "            prev_word2 = sentence[1]\n",
    "            for word in sentence[2:]:\n",
    "                self.trigram_frequencies[(prev_word1, prev_word2, word)] = (\n",
    "                    self.trigram_frequencies.get((prev_word1, prev_word2, word), 0) + 1\n",
    "                )\n",
    "                prev_word1 = prev_word2\n",
    "                prev_word2 = word\n",
    "                self.total_trigrams += 1\n",
    "\n",
    "        self.total_trigram_words = len(self.trigram_frequencies)\n",
    "\n",
    "    def calculate_probability(self, prev_word1, prev_word2, word):\n",
    "        trigram_frequency = self.trigram_frequencies.get(\n",
    "            (prev_word1, prev_word2, word), 0\n",
    "        )\n",
    "        bigram_frequency = self.bigram_frequencies.get((prev_word2, word), 0)\n",
    "\n",
    "        if bigram_frequency == 0:\n",
    "            return 0\n",
    "        return float(trigram_frequency) / float(bigram_frequency)\n",
    "\n",
    "    def calculate_perplexity(self, prev_word1, prev_word2, word):\n",
    "        prob_word = self.calculate_probability(prev_word1, prev_word2, word)\n",
    "        if prob_word == 0:\n",
    "            return 10 ** (7)\n",
    "        perplexity = np.log2(float((1 / prob_word))) * (float(1 / self.total_trigrams))\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_probability_smooth(self, prev_word1, prev_word2, word, k):\n",
    "        trigram_frequency = self.trigram_frequencies.get(\n",
    "            (prev_word1, prev_word2, word), 0\n",
    "        )\n",
    "        bigram_frequency = self.bigram_frequencies.get((prev_word2, word), 0)\n",
    "\n",
    "        return (float(trigram_frequency) + k) / (\n",
    "            float(bigram_frequency) + k * self.vocab_size\n",
    "        )\n",
    "\n",
    "\n",
    "class quadgram_model(trigram_model):\n",
    "    def __init__(self, sentences):\n",
    "        trigram_model.__init__(self, sentences)\n",
    "        self.quadgram_frequencies = {}\n",
    "        self.total_quadgrams = 0\n",
    "        for sentence in sentences:\n",
    "            prev_word1 = sentence[0]\n",
    "            prev_word2 = sentence[1]\n",
    "            prev_word3 = sentence[2]\n",
    "            for word in sentence[3:]:\n",
    "                quadgram = (prev_word1, prev_word2, prev_word3, word)\n",
    "                self.quadgram_frequencies[quadgram] = (\n",
    "                    self.quadgram_frequencies.get(quadgram, 0) + 1\n",
    "                )\n",
    "                prev_word1 = prev_word2\n",
    "                prev_word2 = prev_word3\n",
    "                prev_word3 = word\n",
    "                self.total_quadgrams += 1\n",
    "\n",
    "        self.total_quadgram_words = len(self.quadgram_frequencies)\n",
    "\n",
    "    def calculate_probability(self, prev_word1, prev_word2, prev_word3, word):\n",
    "        quadgram_frequency = self.quadgram_frequencies.get(\n",
    "            (prev_word1, prev_word2, prev_word3, word), 0\n",
    "        )\n",
    "        trigram_frequency = self.trigram_frequencies.get(\n",
    "            (prev_word1, prev_word2, prev_word3), 0\n",
    "        )\n",
    "\n",
    "        if trigram_frequency == 0:\n",
    "            return 0\n",
    "        return float(quadgram_frequency) / float(trigram_frequency)\n",
    "\n",
    "    def calculate_perplexity(self, prev_word1, prev_word2, prev_word3, word):\n",
    "        prob_word = self.calculate_probability(prev_word1, prev_word2, prev_word3, word)\n",
    "        if prob_word == 0:\n",
    "            return 10 ** (7)\n",
    "        perplexity = np.log2(float((1 / prob_word))) * (float(1 / self.total_quadgrams))\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_probability_smooth(self, prev_word1, prev_word2, prev_word3, word, k):\n",
    "        quadgram_frequency = self.quadgram_frequencies.get(\n",
    "            (prev_word1, prev_word2, prev_word3, word), 0\n",
    "        )\n",
    "        trigram_frequency = self.trigram_frequencies.get(\n",
    "            (prev_word1, prev_word2, prev_word3), 0\n",
    "        )\n",
    "\n",
    "        return (float(quadgram_frequency) + k) / (\n",
    "            float(trigram_frequency) + k * self.vocab_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_dataset.csv')\n",
    "test_comments='Comment'\n",
    "\n",
    "test_data[test_comments]=test_data[test_comments].apply(tokenize_sent)\n",
    "test_unigram_data=test_data[test_comments]\n",
    "test_sentences=[]\n",
    "for data in test_unigram_data:\n",
    "    for sent in data:\n",
    "        new_sent=sent.lower()\n",
    "        test_sentences.append(new_sent.translate(translator))\n",
    "\n",
    "test_unigram_sent = test_sentences\n",
    "test_bigram_sent , test_trigram_sent , test_quadgram_sent = test_unigram_sent.copy() , test_unigram_sent.copy(), test_unigram_sent.copy()\n",
    "test_unigram_words=[]\n",
    "test_trigram_words = []\n",
    "test_quadgram_words = []\n",
    "test_bigram_words = []\n",
    "\n",
    "\n",
    "\n",
    "for sent in test_unigram_sent:\n",
    "    words=word_tokenize(sent)\n",
    "    test_unigram_words.append(words)\n",
    "    \n",
    "\n",
    "for sent in test_trigram_sent:\n",
    "    \n",
    "    words=word_tokenize(sent)\n",
    "    words.append('</s>')\n",
    "    words.append('</s>')\n",
    "    words.insert(0,'<s>')\n",
    "    words.insert(0,'<s>')\n",
    "    test_trigram_words.append(words)\n",
    "    \n",
    "for sent in test_bigram_sent:\n",
    "    \n",
    "    words=word_tokenize(sent)\n",
    "    words.append('</s>')\n",
    "    words.insert(0,'<s>')\n",
    "    test_bigram_words.append(words)\n",
    "    \n",
    "for sent in test_quadgram_sent:\n",
    "    \n",
    "    words=word_tokenize(sent)\n",
    "    words.append('</s>')\n",
    "    words.append('</s>')\n",
    "    words.append('</s>')\n",
    "    words.insert(0,'<s>')\n",
    "    words.insert(0,'<s>')\n",
    "    words.insert(0,'<s>')\n",
    "    test_quadgram_words.append(words)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIGRAM_MODEL=unigram_model(unigram_words)\n",
    "\n",
    "count=0\n",
    "total_perplex = 0\n",
    "# UNIGRAM_MODEL.calculate_probability_good_turing()\n",
    "for data in test_unigram_words:\n",
    "\n",
    "    count+=1\n",
    "    n = len(data)\n",
    "    if(n == 0): continue\n",
    "    unigram_perplex=0\n",
    "    for word in data:\n",
    "        unigram_perplex += np.log2(UNIGRAM_MODEL.calculate_probability(word))\n",
    "    \n",
    "    unigram_perplex *= (-1/n)\n",
    "    final_perplex = 2 ** unigram_perplex\n",
    "    total_perplex += final_perplex\n",
    "        \n",
    "        \n",
    "avg_perplex = total_perplex/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11276.294992566758"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3507.8123080565356"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIGRAM_MODEL=bigram_model(bigram_words)\n",
    "total_perplex=0\n",
    "# BIGRAM_MODEL.initialise_kneser_ney()\n",
    "count=0\n",
    "\n",
    "for data in test_unigram_words:\n",
    "    count+= 1\n",
    "    bigram_perplex = 0\n",
    "    n = len(data)\n",
    "    if(n < 2): continue\n",
    "    \n",
    "    for i in range(len(data)-1):\n",
    "        bigram_perplex+=np.log2(BIGRAM_MODEL.calculate_probability(data[i],data[i+1]))\n",
    "        \n",
    "    bigram_perplex *= (-1/n)\n",
    "    final_perplex = 2 ** bigram_perplex\n",
    "    total_perplex += final_perplex\n",
    "        \n",
    "        \n",
    "Avg_perplex=total_perplex/count\n",
    "Avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGRAM_MODEL=trigram_model(trigram_words)\n",
    "total_perplex=0\n",
    "count=0\n",
    "\n",
    "for data in test_unigram_words:\n",
    "    count+= 1\n",
    "    trigram_perplex = 0\n",
    "    n = len(data)\n",
    "    if(n < 3): continue\n",
    "    \n",
    "    for i in range(len(data)-2):\n",
    "        trigram_perplex+=np.log2(TRIGRAM_MODEL.calculate_probability(data[i],data[i+1], data[i+2]))\n",
    "        \n",
    "    trigram_perplex *= (-1/n)\n",
    "    final_perplex = 2 ** trigram_perplex\n",
    "    total_perplex += final_perplex\n",
    "        \n",
    "        \n",
    "Avg_perplex=total_perplex/count\n",
    "Avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUADGRAM_MODEL=quadgram_model(quadgram_words)\n",
    "total_perplex=0\n",
    "count=0\n",
    "\n",
    "for data in test_unigram_words:\n",
    "    count+= 1\n",
    "    quadgram_perplex = 0\n",
    "    n = len(data)\n",
    "    if(n < 4): continue\n",
    "    \n",
    "    for i in range(len(data)-3):\n",
    "        quadgram_perplex+=np.log2(QUADGRAM_MODEL.calculate_probability(data[i],data[i+1], data[i+2] , data[i+3]))\n",
    "        \n",
    "    quadgram_perplex *= (-1/n)\n",
    "    final_perplex = 2 ** quadgram_perplex\n",
    "    total_perplex += final_perplex\n",
    "        \n",
    "        \n",
    "Avg_perplex=total_perplex/count\n",
    "Avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1 , 1 , 1 , endpoint = True)   \n",
    "y = []\n",
    "UNIGRAM_MODEL=unigram_model(unigram_words)\n",
    "\n",
    "for data in test_unigram_words:\n",
    "    for word in data:\n",
    "        if word not in UNIGRAM_MODEL.vocabulary:\n",
    "            UNIGRAM_MODEL.vocabulary.add(word)\n",
    "            UNIGRAM_MODEL.vocab_size += 1\n",
    "            UNIGRAM_MODEL.unigram_frequencies[word] = 0\n",
    "\n",
    "for k in x:\n",
    "    \n",
    "    count=0\n",
    "    total_perplex = 0\n",
    "    for data in test_unigram_words:\n",
    "\n",
    "        count+=1\n",
    "        n = len(data)\n",
    "        if(n == 0): continue\n",
    "        unigram_perplex=0\n",
    "        for word in data:\n",
    "            unigram_perplex += np.log2(UNIGRAM_MODEL.calculate_probability_smooth(word , k) )\n",
    "\n",
    "        unigram_perplex *= (-1/n)\n",
    "        final_perplex = 2 ** unigram_perplex\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "    avg_perplex = total_perplex/count\n",
    "    print(avg_perplex)\n",
    "    y.append(avg_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x , y , color = 'blue')\n",
    "plt.xlabel('Value of k')\n",
    "plt.ylabel('Average Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1 , 1 , 1 , endpoint = True)   \n",
    "y = []\n",
    "BIGRAM_MODEL=bigram_model(bigram_words)\n",
    "\n",
    "for data in test_unigram_sent:\n",
    "    n = len(data)\n",
    "    for word in data:\n",
    "        if word not in BIGRAM_MODEL.vocabulary:\n",
    "            BIGRAM_MODEL.vocabulary.add(word)\n",
    "            BIGRAM_MODEL.vocab_size += 1\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        \n",
    "        if(data[i], data[i+1]) not in BIGRAM_MODEL.bigram_frequencies:\n",
    "            BIGRAM_MODEL.bigram_frequencies[(data[i], data[i+1])] = 0\n",
    "            \n",
    "for k in x:\n",
    "    \n",
    "    total_perplex=0\n",
    "    count=0\n",
    "\n",
    "    for data in test_unigram_words:\n",
    "        count+= 1\n",
    "        bigram_perplex = 0\n",
    "        n = len(data)\n",
    "        if(n < 2): continue\n",
    "\n",
    "        for i in range(len(data)-1):\n",
    "            \n",
    "            bigram_perplex+=np.log2(BIGRAM_MODEL.calculate_probability_smooth(data[i],data[i+1] , k))\n",
    "\n",
    "        bigram_perplex *= (-1/n)\n",
    "        final_perplex = 2 ** bigram_perplex\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "\n",
    "    Avg_perplex=total_perplex/count\n",
    "    y.append(Avg_perplex)\n",
    "    print(Avg_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x , y , color = 'blue')\n",
    "plt.xlabel('Value of k')\n",
    "plt.ylabel('Average Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGRAM_MODEL=trigram_model(trigram_words)\n",
    "\n",
    "y = []\n",
    "for data in test_unigram_sent:\n",
    "    n = len(data)\n",
    "    for word in data:\n",
    "        if word not in TRIGRAM_MODEL.vocabulary:\n",
    "            TRIGRAM_MODEL.vocabulary.add(word)\n",
    "            TRIGRAM_MODEL.vocab_size += 1\n",
    "    \n",
    "    for i in range(n-2):\n",
    "        if(data[i], data[i+1], data[i+2]) not in TRIGRAM_MODEL.trigram_frequencies:\n",
    "            TRIGRAM_MODEL.trigram_frequencies[(data[i], data[i+1] , data[i+2])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1 , 1 , 1 , endpoint = True)  \n",
    "y = []\n",
    "for k in x:\n",
    "    \n",
    "    total_perplex=0\n",
    "    count=0\n",
    "\n",
    "    for data in test_unigram_words:\n",
    "        count+= 1\n",
    "        trigram_perplex = 0\n",
    "        n = len(data)\n",
    "        if(n < 3): continue\n",
    "\n",
    "        for i in range(len(data)-2):\n",
    "            trigram_perplex+=np.log2(TRIGRAM_MODEL.calculate_probability_smooth(data[i],data[i+1], data[i+2] ,k))\n",
    "\n",
    "        trigram_perplex *= (-1/n)\n",
    "        final_perplex = 2 ** trigram_perplex\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "\n",
    "    Avg_perplex=total_perplex/count\n",
    "    print(Avg_perplex)\n",
    "    y.append(Avg_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x , y , color = 'blue')\n",
    "plt.xlabel('Value of k')\n",
    "plt.ylabel('Average Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUADGRAM_MODEL= quadgram_model(quadgram_words)\n",
    "\n",
    "y = []\n",
    "for data in test_unigram_sent:\n",
    "    n = len(data)\n",
    "    for word in data:\n",
    "        if word not in QUADGRAM_MODEL.vocabulary:\n",
    "            QUADGRAM_MODEL.vocabulary.add(word)\n",
    "            QUADGRAM_MODEL.vocab_size += 1\n",
    "    \n",
    "    for i in range(n-3):\n",
    "        if(data[i], data[i+1], data[i+2] , data[i+3]) not in QUADGRAM_MODEL.quadgram_frequencies:\n",
    "            QUADGRAM_MODEL.quadgram_frequencies[(data[i], data[i+1] , data[i+2] , data[i+3])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1 , 1 , 1 , endpoint = True)  \n",
    "y = []\n",
    "for k in x:\n",
    "    \n",
    "    total_perplex=0\n",
    "    count=0\n",
    "\n",
    "    for data in test_unigram_words:\n",
    "        count+= 1\n",
    "        quadgram_perplex = 0\n",
    "        n = len(data)\n",
    "        if(n < 4): continue\n",
    "\n",
    "        for i in range(len(data)-3):\n",
    "            quadgram_perplex+=np.log2(QUADGRAM_MODEL.calculate_probability_smooth(data[i],data[i+1], data[i+2],data[i+3] ,k))\n",
    "\n",
    "        quadgram_perplex *= (-1/n)\n",
    "        final_perplex = 2 ** quadgram_perplex\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "\n",
    "    Avg_perplex=total_perplex/count\n",
    "    print(Avg_perplex)\n",
    "    y.append(Avg_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Gram(unigram_model):\n",
    "    def __init__(self,sentences,n):\n",
    "        if(n==1):\n",
    "            self.model=unigram_model(sentences)\n",
    "        if(n==2):\n",
    "            self.model=bigram_model(sentences)\n",
    "        if(n==3):\n",
    "            self.model=trigram_model(sentences)\n",
    "        if(n==4):\n",
    "            self.model=quadgram_model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uni_gram_model=N_Gram(unigram_words,1)\n",
    "test_bi_gram_model=N_Gram(bigram_words,2)\n",
    "test_tri_gram_model=N_Gram(trigram_words,3)\n",
    "test_quad_gram_model=N_Gram(quadgram_words,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_uni_gram_model.model.calculate_probability('the'))\n",
    "print(test_bi_gram_model.model.calculate_probability('is','the'))\n",
    "print(test_tri_gram_model.model.calculate_probability('seems','to','me'))\n",
    "print(test_quad_gram_model.model.calculate_probability('seems','to','me','that'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quad_gram_model= N_Gram(quadgram_words,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
